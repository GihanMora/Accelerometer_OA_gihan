from __future__ import print_function

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

from os import listdir, makedirs
from os.path import join, isfile, exists
import pickle
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
# import seaborn as sns
import shutil
from tqdm import tqdm
from time import time
import keras
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Dropout, Reshape, GlobalMaxPooling1D
from tensorflow.keras.layers import Conv1D, MaxPooling1D
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import tensorflow.keras
# import np_utils
from tensorflow.keras.callbacks import TensorBoard

from sklearn.metrics import confusion_matrix
# import statistical_extensions as SE
import random
import sys

from scipy.stats.stats import pearsonr
from sklearn.metrics import explained_variance_score, mean_squared_error, r2_score, confusion_matrix
# -*- coding: utf-8 -*-
"""CNN_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eyc--ceh4kX8_eYdBJ_o-yqFlAiwZrrf
"""





import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

from os import listdir, makedirs
from os.path import join, isfile, exists
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
import shutil
from tqdm import tqdm
from time import time
import keras
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Dropout, Reshape, GlobalMaxPooling1D
from tensorflow.keras.layers import Conv1D, MaxPooling1D
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.callbacks import TensorBoard
# tboard_log_dir = os.path.join("logs",'mylog')
# tensorboard = TensorBoard(log_dir = tboard_log_dir)
# import statistical_extensions as SE

# sys.path.append('F:/OA/evaluate_results')

# test_ID_user = []
# test_Y_data = []
# y_pred_test= []
# c = 0
# # Set a length of the list to 10
# for i in range(0, 10000):
#     # any random float between 50.50 to 500.50
#     # don't use round() if you need number as it is
#
#     x = round(random.uniform(0.50, 5.50), 2)
#     y = round(random.uniform(0.50, 5.50), 2)
#     test_Y_data.append(x)
#     y_pred_test.append(y)
#     test_ID_user.append(c)
#     c += 1
#
#
# # results_df = pd.DataFrame()
# # results_df['subject'] = test_ID_user
# # results_df['waist_ee'] = test_Y_data
# # results_df['predicted_ee'] = y_pred_test
#
#
# results_df = pd.DataFrame(
#     {'subject': test_ID_user,
#       'waist_ee': test_Y_data,
#       'predicted_ee': y_pred_test
#       })
# # print('ssss',results_df['waist_ee'].shape)
# def clean_data_points(data):
#     data = data.assign(waist_ee_cleaned=data['waist_ee'])
#     data = data.assign(predicted_ee_cleaned=data['predicted_ee'])
#     data.loc[(data['predicted_ee'] < 1), 'predicted_ee_cleaned'] = 1
#     return data
#
# results_df = clean_data_points(results_df)
# import statistical_extensions_1 as SE1
# SE1.BlandAltman.bland_altman_paired_plot_tested(results_df, 'FOLDER_NAME', 1, log_transformed=True,
#                                                 min_count_regularise=False, output_filename=join('F:/OA_CNN/Model_outputs', 'bland_altman'))

def load_data(filenames):

    X_data = []
    Y_data = []
    ID_user = []
    counter = 0
    for filename in tqdm(filenames):
        npy = np.load(filename, allow_pickle=True)
        X_data.append(npy.item().get('segments'))
        Y_data.append(npy.item().get('activity_classes'))

        user_id = filename.split('/')[-1][:6]
        data_length = npy.item().get('activity_classes').shape[0]
        ID_user.extend([user_id for _ in range(data_length)])

        # counter += 1
        # if counter > 10:
        #     break

    X_data = np.concatenate(X_data, axis=0)
    Y_data = np.concatenate(Y_data, axis=0)
    # print('yydata', Y_data)
    # Data relabeling from index 0 (use only 3 classes)
    Y_data = np.where(Y_data == 1, 0, Y_data)
    Y_data = np.where(Y_data == 2, 1, Y_data)
    Y_data = np.where(Y_data == 3, 2, Y_data)
    Y_data = np.where(Y_data == 4, 2, Y_data)
    # print('yydata',Y_data)
    return X_data, Y_data, ID_user


# training_dataset_path = 'E:/Data/Accelerometer_Dataset_Rashmika/OA_data/supervised_data/ActiGraph/numpy_window-3600-overlap-0_train/'
# training_dataset_path = 'E:/Data/Accelerometer_Dataset_Rashmika/OA_data/supervised_data/ActiGraph/numpy_window-3600-overlap-0_train/'
training_dataset_path = 'E:/Data/Accelerometer_Dataset_Rashmika/OA_data/supervised_data/ActivPAL/balanced/numpy_window-300-overlap-150/'
training_data_files = [join(training_dataset_path, f) for f in listdir(training_dataset_path) if isfile(join(training_dataset_path, f))]

print(training_data_files)
train_X_data, train_Y_data, train_ID_user = load_data(training_data_files)
X_train, y_train, ID_train = train_X_data, train_Y_data, train_ID_user


import collections
num_classes = len(np.unique(y_train))
y_train_1 = y_train.tolist()
ctr = collections.Counter(y_train_1)
print("Frequency of the elements in the List : ",ctr)



#
# to_remove = []
# for i,k in enumerate(y_train_1):
#     # print(i,k)
#     if(k==0):
#         to_remove.append(i)
#
# print(to_remove)
# print(len(to_remove))
#
# for j in to_remove[:50]:
#     print(j,'removing')
#     np.delete(X_train,j)
#     np.delete(y_train, j)
#     np.delete(ID_train, j)
#
#
#
#
# import collections
# num_classes = len(np.unique(y_train))
# y_train_1 = y_train.tolist()
# ctr = collections.Counter(y_train_1)
# print("Frequency of the elements in the List : ",ctr)
# test_dataset_path = 'E:/Data/Accelerometer_Dataset_Rashmika/OA_data/supervised_data/ActivPAL/numpy_window-300-overlap-0_test/'
# test_data_files = [join(test_dataset_path, f) for f in listdir(test_dataset_path) if isfile(join(test_dataset_path, f))]
# print(test_data_files)
# test_X_data, test_Y_data, test_ID_user = load_data(test_data_files[:100])
# Counter({0: 421394, 1: 107508, 2: 9271})
# Counter({0: 1699575, 1: 184721, 2: 24952})