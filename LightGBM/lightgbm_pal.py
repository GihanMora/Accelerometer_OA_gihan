# -*- coding: utf-8 -*-
"""LightGBM_pal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xThyw1eP9-SMe9aWe-kLQF9PEjF2NxhP
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install lightgbm
from google.colab import drive
drive.flush_and_unmount()
drive.mount('/content/drive')
# %ls
# %cd drive
# %ls

from google.colab import drive
drive.mount('/content/drive')
import sys
sys.path.append('/content/drive/My Drive/lightGBM/')
# !cp "/content/drive/My Drive/lightGBM/statistical_extensions.py"
import statistical_extensions as SE

# !cp "/content/drive/My Drive/lightGBM/statistical_extensions_1.py"
import statistical_extensions_1 as SE1

import os
from os import listdir, makedirs
from os.path import join, isfile, isdir, exists
from datetime import datetime
import pandas as pd
import numpy as np
import math
from tqdm import tqdm
Actipal_30s_epoch_proc_folder = '/content/drive/My Drive/OA activity data_La Trobe/Analytics_Scripts/Gihan_analysis/processed data/montoye_thigh/Epoch30'
epoch_files = [f for f in listdir(Actipal_30s_epoch_proc_folder) if isfile(join(Actipal_30s_epoch_proc_folder, f))]
print(epoch_files)

X_data = pd.DataFrame()
Y_data = pd.DataFrame()

ID_user = []
count = 0
for filename in tqdm(epoch_files):
  key = filename.split('_')[0]
  dataframe = pd.read_csv(Actipal_30s_epoch_proc_folder+'/'+filename)
  x_data_temp = dataframe[['raw_XMean', 'raw_XMean', 'raw_YMean',
       'raw_ZMean', 'raw_XVar', 'raw_YVar', 'raw_ZVar', 'raw_X_cov',
       'raw_Y_cov', 'raw_Z_cov', 'raw_Xmin', 'raw_Ymin', 'raw_Zmin',
       'raw_Xmax', 'raw_Ymax', 'raw_Zmax', 'raw_orientation_roll_mean',
       'raw_orientation_roll_var', 'raw_orientation_pitch_mean',
       'raw_orientation_pitch_var','raw_X10perc', 'raw_X25perc', 'raw_X50perc', 'raw_X75perc',
       'raw_X90perc', 'raw_Y10perc', 'raw_Y25perc', 'raw_Y50perc',
       'raw_Y75perc', 'raw_Y90perc', 'raw_Z10perc', 'raw_Z25perc',
       'raw_Z50perc', 'raw_Z75perc', 'raw_Z90perc', 'mvm']]

  y_data_temp = dataframe[['thigh_ee_final']]
  # print(dataframe.columns)
  X_data = pd.concat([X_data, x_data_temp])
  Y_data = pd.concat([Y_data, y_data_temp])
  # print(key)
  # count+=1
  # if(count==100):
  #   break

print('***')
print(len(X_data))
print(len(Y_data))
X_data_1 = X_data.iloc[ :900000]
X_pred = X_data.iloc[900000:]
Y_data_1 = Y_data.iloc[:900000]
Y_pred = Y_data.iloc[900000:]

# print('xx',(X_data_1))
# print('xp',(X_pred))
# print('yy',(Y_data_1))
# print('yp',(Y_pred))

import lightgbm as lgb
import numpy as np
import numpy as np
import pandas as pd
import lightgbm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer



#
# Prepare the data
#
# X_pred = X_pred
# X_data = X_data_1
# Y_pred = Y_pred
# Y_data = Y_data_1


print('xxxx',X_pred.head())
print('xxdd',X_data_1.head())
train = X_data_1
# train = pd.read_csv('/content/drive/My Drive/lightGBM/train.csv')
# print('lennnnn',len(train[:100000]))

# get the labels
# y = train.target.values
# print(Y_data['waist_ee'].values)

# y = [list(i)[0] for i in list(Y_data)]
y=Y_data_1['thigh_ee_final'].values
# train.drop(['id', 'target'], inplace=True, axis=1)
# print(y)
x = train.values

#
# Create training and validation sets
#
x, x_test, y, y_test = train_test_split(x, y, test_size=0.2, random_state=41)
print('xxxx',len(x))
print('xxtt',len(x_test))
print('yyyy',len(y))
print('yytt',len(y_test))
#
# Create the LightGBM data containers
#
# categorical_features = [c for c, col in enumerate(train.columns) if 'cat' in col]
# train_data = lightgbm.Dataset(x, label=y, categorical_feature=categorical_features)
train_data = lightgbm.Dataset(x, label=y)
test_data = lightgbm.Dataset(x_test, label=y_test)
# print('ttt',train_data)

#
# Train the model
#

parameters = {
  'task': 'train',
    'boosting_type': 'gbdt',
    'objective': 'regression',
    'metric': ['l2', 'rmse'],
    'learning_rate': 0.05,
    'feature_fraction': 0.9,
    'bagging_fraction': 0.7,
    'bagging_freq': 10,
    'verbose': 0,
    "max_depth": 8,
    "num_leaves": 1024,  
    "max_bin": 512,
    "num_iterations": 800,
    "n_estimators": 1000
        
}

model = lightgbm.train(parameters,
                       train_data,
                       valid_sets=test_data,
                       num_boost_round=5000,
                       early_stopping_rounds=1000)
#
# Create a submission
#

# submission = pd.read_csv('/content/drive/My Drive/lightGBM/test.csv')[:500]
# ids = submission['id'].values
# submission.drop('id', inplace=True, axis=1)


x = X_pred.values
# print(x)
y = model.predict(x)
# print(y)
# print(len(y),len(Y_pred))
# print(Y_pred.values)
output = pd.DataFrame()
output['pred'] = y
output['actual'] = Y_pred['thigh_ee_final'].values
output.to_csv("/content/drive/My Drive/lightGBM/Model_outputs_pal/predictions_vs_reference.csv", index=False)

from scipy.stats.stats import pearsonr
from sklearn.metrics import explained_variance_score, mean_squared_error, r2_score, confusion_matrix
from matplotlib import pyplot as plt
import statistical_extensions as SE

test_Y_data = Y_pred['thigh_ee_final'].values
y_pred_test_1d_list = y

plt.figure(figsize=(8, 8))
plt.scatter(test_Y_data, y_pred_test_1d_list)
plt.xlabel('Actual EE')
plt.ylabel('Predicted EE')
plt.savefig(join('/content/drive/My Drive/lightGBM/Model_outputs_pal', 'actual_vs_predicted_met.png'))
plt.clf()
plt.close()

corr = pearsonr(test_Y_data, y_pred_test_1d_list)
grp_results = []
grp_results.append('\n\n -------RESULTS-------\n\n')
grp_results.append('Pearsons Correlation = {}'.format(corr))
grp_results.append('RMSE - {}'.format(np.sqrt(mean_squared_error(test_Y_data, y_pred_test_1d_list))))
grp_results.append('R2 Error - {}'.format(r2_score(test_Y_data, y_pred_test_1d_list)))
grp_results.append('Explained Variance Score - {}'.format(explained_variance_score(test_Y_data, y_pred_test_1d_list)))

class_names = ['SED', 'LPA', 'MVPA']
y_test_ai = SE.EnergyTransform.met_to_intensity(test_Y_data)
y_pred_test_ai = SE.EnergyTransform.met_to_intensity(y_pred_test_1d_list)

cnf_matrix = confusion_matrix(y_test_ai, y_pred_test_ai)

stats = SE.GeneralStats.evaluation_statistics(cnf_matrix)

assessment_result = 'Classes' + '\t' + str(class_names) + '\t' + '\n'
assessment_result += 'Accuracy' + '\t' + str(stats['accuracy']) + '\t' + str(stats['accuracy_ci']) + '\n'
assessment_result += 'Sensitivity' + '\t' + str(stats['sensitivity']) + '\n'
assessment_result += 'Sensitivity CI' + '\t' + str(stats['sensitivity_ci']) + '\n'
assessment_result += 'Specificity' + '\t' + str(stats['specificity']) + '\n'
assessment_result += 'Specificity CI' + '\t' + str(stats['specificity_ci']) + '\n'

grp_results.append(assessment_result)

SE.GeneralStats.plot_confusion_matrix(cnf_matrix, classes=class_names, title='CM',
                                      output_filename=join('/content/drive/My Drive/lightGBM/Model_outputs_pal/', 'confusion_matrix.png'))
test_ID_user = [k for k in range(0,len(test_Y_data))]
results_df = pd.DataFrame(
    {
        'subject': test_ID_user,
      'waist_ee': test_Y_data,
      'predicted_ee': y_pred_test_1d_list
      })

def clean_data_points(data):
    data = data.assign(waist_ee_cleaned=data['waist_ee'])
    data = data.assign(predicted_ee_cleaned=data['predicted_ee'])
    data.loc[(data['predicted_ee'] < 1), 'predicted_ee_cleaned'] = 1
    return data

results_df = clean_data_points(results_df)





# SE1.BlandAltman.bland_altman_paired_plot_tested(results_df, 'FOLDER_NAME', 1, log_transformed=True,
#                                                 min_count_regularise=False, output_filename=join('/content/drive/My Drive/CNN/Model_outputs', 'bland_altman'))

result_string = '\n'.join(grp_results)
with open('/content/drive/My Drive/lightGBM/Model_outputs_pal/result_report.txt', "w") as text_file:
    text_file.write(result_string)

# Input
root_folder = '/content/drive/My Drive/OA activity data_La Trobe'
ActiGraph_60s_epoch_proc_folder = '/content/drive/My Drive/OA activity data_La Trobe/Analytics_Scripts/Rashmika_analysis/processed_data/actigraph/epoch_data/60s_ref'
ActiGraph_60hz_raw_folder = '/content/drive/My Drive/left_graph_raw'

# sleep time input_data
sleep_time_file = '/content/drive/My Drive/OA activity data_La Trobe/Analytics_Scripts/Rashmika_analysis/metadata/record_availability_final.csv'

# Output
Actigraph_output_folder = '/content/drive/My Drive/processed data/hibbing/'